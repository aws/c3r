// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0

package com.amazonaws.c3r.spark.io.schema;

import com.amazonaws.c3r.config.ClientSettings;
import com.amazonaws.c3r.config.ColumnHeader;
import com.amazonaws.c3r.data.ClientDataType;
import com.amazonaws.c3r.spark.io.parquet.SparkParquetReader;
import com.amazonaws.c3r.utils.FileUtil;
import lombok.Builder;
import lombok.NonNull;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.DataTypes;

import java.util.Arrays;
import java.util.stream.Collectors;

/**
 * Used to generate a schema file for a specific Parquet file. User can ask for either a simple, autogenerated schema or be walked through
 * the entire schema creation process.
 */
public final class ParquetSchemaGenerator extends SchemaGenerator {
    /**
     * Set up for schema generation and validate settings.
     *
     * @param inputParquetFile Parquet file to read header information from
     * @param targetJsonFile   Where to save the schema
     * @param overwrite        Whether the {@code targetJsonFile} should be overwritten (if it exists)
     * @param clientSettings   Collaboration's client settings if provided, else {@code null}
     * @param sparkSession     SparkSession to use for sampling the input schema
     */
    @Builder
    private ParquetSchemaGenerator(@NonNull final String inputParquetFile,
                                   @NonNull final String targetJsonFile,
                                   @NonNull final Boolean overwrite,
                                   final ClientSettings clientSettings,
                                   @NonNull final SparkSession sparkSession) {
        super(inputParquetFile, targetJsonFile, overwrite, clientSettings);
        FileUtil.verifyReadableFile(inputParquetFile);
        final Dataset<Row> dataset = SparkParquetReader.readInput(sparkSession, inputParquetFile);
        final String[] headers = dataset.columns();
        sourceHeaders = Arrays.stream(headers).map(ColumnHeader::new).collect(Collectors.toList());
        sourceColumnTypes = Arrays.stream(dataset.schema().fields())
                .map(field -> {
                    if (field.dataType() == DataTypes.StringType) {
                        return ClientDataType.STRING;
                    } else {
                        return ClientDataType.UNKNOWN;
                    }
                }).collect(Collectors.toList());
    }

}
